{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Posts into Relevant Channels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "\n",
    "#import itertools\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "#from glove import Corpus, Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the trained word2vec model\n",
    "\n",
    "#### word_vectors = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True,limit=100000)\n",
    "\n",
    "#### After dumping into a pickle file: model_nlp, we directly operate over it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_nlp','rb') as f:\n",
    "    word_vectors=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning File using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Akanksha\n",
      "[nltk_data]     Tanwar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "text_linked=\"\"\"Ready to be inspired?Applications for the 2021 Generation Google Scholarship:\n",
    "for women in computer science (APAC) are now open\n",
    "Learn more about eligibility and how you can apply at http://goo.gle/3kidA0l\n",
    "Generation Google Scholarship: for women in computer science (formerly known as \n",
    "Women Techmakers Scholarship) aims to inspire and support students in the fields of computing and technology and become active role models and leaders in the industry. As a scholar, you will receive a cash award for the 2021-2022 academic year and be invited to attend the annual (virtual) Scholars' Retreat to connect with fellow scholars, network with Googlers and participate in a number of developmental workshops to help enhance your skills to prepare you for a better tomorrow.\n",
    "Still on the fence?\n",
    "Google is hosting an Info Session over Youtube Livestream on Monday, 15th March 2021 where you will get to meet former scholars and ask live questions! Sign up now at http://goo.gle/3uqUkCu\n",
    "Want to learn more about the essay section? Check out my Blog on Medium :) - https://lnkd.in/eC4nU-x\n",
    "Application closing on Monday, 29th March 2021.\"\"\"\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "stopwords = stopwords.words('english')\n",
    "stopwords.append('\\n')\n",
    "\n",
    "def get_processed_text(text):\n",
    "    \"\"\"\n",
    "    input: text -> an entire string of text\n",
    "    output: tokens -> a list containing all filtered words\n",
    "    \"\"\"\n",
    "    tags = re.compile(r'<.*?>')\n",
    "    tags.sub('', text)                                 # to remove content in HTML tags\n",
    "    text = re.sub(r'http\\S+', ' ', text)               # to remove URLs\n",
    "    text = re.sub(r'[^\\w\\s]',' ', text)                 # to remove punctuations\n",
    "    text = re.sub(r'[^a-zA-Z]', ' ', text).lower()     # to remove anything other than characters\n",
    "    tokens = [w for w in w_tokenizer.tokenize(text) if w not in stopwords and w[0] != '@'] # tokenizing across whitsepaces to extract words\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hashtags\n",
    "categories=['women','job','coding','competition','scholarships','mentors','commerce','law','arts','creative','digital','session','event','finance']\n",
    "match_of_categories=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset=get_processed_text(text_linked)\n",
    "\n",
    "for i in new_dataset:\n",
    "    i=i.lower()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_one_out(words):\n",
    "    \"\"\"Accepts a list of words and returns the odd word\"\"\"\n",
    "    match_of_categories.clear()\n",
    "   \n",
    "    for x in categories: \n",
    "        \n",
    "        same_word=0\n",
    "        \n",
    "        try:\n",
    "            avg_vector = word_vectors[x]\n",
    "            \n",
    "        except:\n",
    "            for w in words:\n",
    "                if(w==x):\n",
    "                    match_of_categories.append(1)\n",
    "                    same_word=1\n",
    "                    \n",
    "            if(same_word==1):\n",
    "                continue\n",
    "                \n",
    "                \n",
    "        #Iterate over every word and find similarity\n",
    "        \n",
    "        min_similarity = 0.5 #Very high value\n",
    "        \n",
    "        flag=0;\n",
    "        \n",
    "        words = new_dataset\n",
    "        \n",
    "        first_10=[]\n",
    "        first_10.clear()\n",
    "\n",
    "        for w in words:  \n",
    "            try:\n",
    "                temp=word_vectors[w]\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            sim = cosine_similarity([temp],[avg_vector])\n",
    "\n",
    "            sim=float(sim)\n",
    "            \n",
    "            if sim>=1.0:\n",
    "                match_of_categories.append(1)\n",
    "                flag=1\n",
    "                break\n",
    "            \n",
    "            if sim>=0.5:\n",
    "                t=[w,sim]\n",
    "                first_10.append(t)\n",
    "                \n",
    "        if len(first_10)>=2 and flag==0:\n",
    "            match_of_categories.append(1)\n",
    "            \n",
    "        elif len(first_10)<1 and flag==0:\n",
    "            match_of_categories.append(0)\n",
    "            \n",
    "        first_10.clear()\n",
    "            \n",
    "    return match_of_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy=similar_one_out(new_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['women', 'job', 'coding', 'competition', 'scholarships', 'mentors', 'commerce', 'law', 'arts', 'creative', 'digital', 'session', 'event', 'finance']\n",
      "[1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(categories)\n",
    "print(copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
